\documentclass[a4paper,10pt]{report}

\usepackage[english]{babel}
%\usepackage[none]{hyphenat}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage[pdftex]{hyperref}
%\usepackage[tight,scriptsize]{subfigure}
%\usepackage{ucs}
%\usepackage[utf8x]{inputenc}
\usepackage{jfc}
\hypersetup{hypertexnames, plainpages=false,colorlinks=true}
\usepackage{amsmath}

% Title Page
\title{}
\author{}

\usepackage{xr}
\externaldocument{../edge-detection-review-2nd-revision-v2}
\reversemarginpar

\newcounter{myquestion}
\newcommand{\ml}{\mathcal{L}}
\newcommand{\ms}{\mathcal{S}}

\begin{document}
\maketitle

%\begin{abstract}
%\end{abstract}

\chapter{Introduction}
In this communication we answer to the second review of the article "A Contrario Selection of Optimal Partitions for Image Segmentation", submitted to the SIAM Journal on Imaging Sciences.
In the first place, we would like to thank again the reviewers for the detailed and thoughtful review provided. In the following we address the issues raised by the reviewers. 
For each of them, an answer is provided and a cross-reference with a red tag Cn (where n is the number of the correction) to the corresponding (corrected) part of the paper is provided. In addition, changes made to the paper are highlighted in red (when significative) in the new version of the paper. 
These red marks will be removed for the final version.

The remarks from the reviewers are marked with the tag \textcolor{blue}{QUESTION} and our answers with the tag \ans.

%\section{Brief summary of changes}
\section{General changes}
\begin{itemize}
 \item Revised general writing of the article.
\item We added appendices to include secondary information that distracts from the main purpose of the article.
\item In that sense we moved the numeric values of the $\mathbf{mask_i}$ matrices to the first appendix
\item We also moved the Implementation section, which documents the code, to the second appendix.
\item Removed several typos and orthographic errors.
\end{itemize}


\chapter{Answers to reviewer \#1}

\section{Summary}

\que This paper reviews classic edge detectors based on the first and second derivatives of the image. It focuses
on three first-derivative based algorithms (namely Sobel, Prewitt and Roberts) and two second-derivative
based algorithms (Marr-Hildreth and Haralick). The paper itself explains all the implementation details.
The code provides the corresponding implementation.\\

\section{Paper review}

\que In general the paper provides all the information necessary. Some parts need nevertheless rewriting either because of minor mistakes (that make some sentences really hard to read) or because of mathematical inaccuracies.\\

\ans We thank the reviewer for the comments and for contributing to improve the quality of the paper. We did a major proof-reading of the paper and we did our best effort to improve the writing.\\

\que You mention a video but it was not included in the supplementary les (35-757-1-SP.zip and 35-758-1-SP.zip) so that I could not see it.\\

\ans XXX

\que p2: There is no need to define x = ih and y = ih, you should take x = i; y = j, and then add a small h, which will be equal to 1 since you are on a grid. In theory the approximation is valid for h small, thus it is really questionable to define x = ih? \\

\ans This is a confusion, we should present the taylor approximation of the derivative using h, and then define the grid sizes as $\Delta x$ and $\Delta y$, and thus define $x=i\Delta x$. This is done in Correcction \ref{correction_a:first_derivative:discretization}. Next paragraph was also rewritten, see \refc{correction_a:first_derivative:diagonal edges}\\

\que p4: The computational diference between Prewitt and Sobel masks is not important" -> You need to justify this sentence. Show and refer to an experiment of some kind. Otherwise this is an unfounded claim. \\

\ans We think we did not explain ourselves correctly. We meant that the computational cost of both filters is the same, thus we are free to pick the more accurate one. To claim that is better to filter with a gaussian filter than a moving average is a well established practice in the signal processing community.\\

\que Algorithm 2: lines 2 and 3 need to be moved inside the else statement (between lines 7 and 8) otherwise you are computing something that is not always used.\\

\ans Fixed  in \refc{correction_a:second_derivative:marr_algo}.\\

\que p10: the $b_i$ you are defining are referred as $mask_i$ in algorithm 3. It is actually better to name them
$mask_i$ to avoid confusion with $b_{i,j}$ . It is also inaccurate to state where $bi =...$ since you have not
used the $b_i$ yet but the $b_{i,j}$ . It makes the whole thing very hard to read.\\

\ans While we agree on making the notation coherent, we prefer to keep the name $b_i$. Naming a matrix $B$, its rows as $\mathbf{b_i}$ and its elements $b_{i,j}$ it is a quite common (and unambiguous) way to use the notation. However we also added $\mathbf{mask_i}$ and defined it as being $\mathbf{b_i}$ reordered. Thus, we kept algorithm 3 using $mask_i$ (see \refc{correction_a:second_derivative:haralick_algo}), we defined $\mathbf{mask_i}$ and we used $\mathbf{b_i}$ before saying where. Finally, we stated explicitely that $\mathbf{b_i}$ is a vector containing the $i_{th} $ row of matrix $B$ (See \refc{correction_a:second_derivative:haralick_bi}).

\que Can you add an experiment showing the influence of the padding method on the results?\\

\que Use a "Name +year" bibliography as asked in the IPOL writing guidelines.\\
4 points to ipol, In addition to that it is not referred to in the text.\\

\ans This was corrected in two ways. As new IPOL guidelines suggest, the SIAM bibliography style was applied. Reference [4] now points to the correct article.\\

\section{minor remarks}

\que In general I would recommend proofreading the whole paper, there are a lot of grammar
mistakes that make the paper hard to follow, I noted some of them below but this list is not exhaustive at all.\\

\ans We thank the reviewer for the comment, most of the paper has been revised thoroughly. Specially the points remarked in this section.\\

\que Algorithms based onfirst derivative \\
\ans The title of the section and the first paragraph of it was rewritten to improve grammar. See  \refc{correction_a:first_derivative:first_deriv_rephrase}\\

\que Taylor expansion of first order with a small h\\

\ans Rephrased in \refc{correction_a:first_derivative:taylor}\\

\que p3 diferences\\

\ans Fixed in \refc{correction_a:typo}. \\

\que "normalization" do you mean "up to the normalization"\\

\ans Corrected in \ref{correction_a:first_derivative:sobel} and \ref{correction_a:first_derivative:sobel}.\\

\que "thus It"\\

\ans Fixed\\

\que "As mentioned before this leads to ab overdetermied"

\ans Fixed\\ 

\que "Gaussian kernel is generated by sampling the"\\
\que "unlike the case of Gaussian kernel"\\
\que "are not independent of each one"\\
\que "implies complete the borders with many zero valued pixels as needed"\\
\que "It can be seen in gure 16 an area of interest of the image."\\
\que "appearing unwanted isolated edges"\\
\que "because they only consists"\\
\que "more fair"\\

\que "runs significative faster"\\

\ans Fixed\\

\que "polinomial"\\
\ans Fixed\\

\chapter{Answers to reviewer \#2}

\section{General comments}
Overall I found the article clear (only the Haralick algorithm explanations are
confusing). The code is very clear, well documented (although not following the Doxygen conventions) and matches almost perfectly
the description in the pdf. Finally the submitted demo works as expected with no
more parameters than the ones presented in the paper. All this makes the
submission a very good one for Ipol.

However, I think that the wording is largely below standard. I have reported
some of the typos, but beyond that, several sentences are wrongly formulated and
even entire paragraphs should be restructured. I have proposed some improvements, yet
I think that the authors should take upon themselves to globally improve the
quality of the paper in this regard.

\section{Comments on the PDF}
\subsection{General description of the article}
This article describes classical algorithms for edge detection. First the
category of first order derivative is described. The only degree of freedom
considered in this article for this class of algorithms lies in the operator
used to approximate the derivative. The article includes some details on the
Robert operators, the Prewitt operators and the Sobel operators. Otherwise the
algorithms are kept strictly identical and are composed of:

- a filtering to compute the image gradient
- a thresholding on the magnitude of the gradient to select edges

Each step is described within the text with sufficient details and a consistent
pseudo-code for the entire algorithms is provided at the end of the
corresponding section.

In a subsequent section, two algorithms based on second order derivatives are
described: the Marr-Hildreth algorithm as well as the alternative proposed by
Haralick. Both algorithms are "conceptually" composed of two steps:

- compute the second order derivatives
- find the zero crossing locations and set them as edges

Once again the two algorithms differs mainly in the way of computing
derivatives. The former rely on the filtering with laplacian of gaussian or on
the filtering with a gaussian followed by a laplacian operator. The latter
relies on a local bicubic approximation followed by a second derivative
analytical computation. Sufficient details are again provided along with
algorithmic description. However some of the details could be made clearer (see
later comments).

The fourth section provides technical details on some filtering aspects and the
last section is dedicated to the experimental discussion.


\subsection{Detailed comments, typos and typesetting}
\subsubsection{Demo}
The url \url{http://dev.ipol.im/~haldos/ipol_demo/xxx_edges/} provided in the preprint
contains a broken demo. This is not very important though since the demo code
provided in the submission works correctly and it should eventually replace the
one specified in the pdf.


\subsubsection{ Section 2.3: The Sobel operators}
No upper case on the "i" of "thus It".


\subsubsection{Section 2.4: Computation of the edges}
The threshold variable "th" should be in italic.
Missing a "." at the end of the section.


\subsubsection{Algorithm 2:}
\begin{verbatim}
tzc" --> "th_{ZC}"
\end{verbatim}

\subsubsection{ Section 3.2.1: Bi-cubic polynomial fitting}

\que I would put more emphasis on the fact that the approximation is expressed in a
local frame, so that the pixel coordinates $x_1,y_1\dots,x_{25},y_{25}$ are
always the same. This is implicitly hinted by the following sentence:
"e.g. in a neighborhood of size 5 × 5, x and y take values between −2 and 2".
But I would insist on this fact again at the end, otherwise it is not clear why
the matrix $B$ can be precomputed once for all.\\

\ans This was taken into account and the entire section was rewritten. Now it is said at the begginning and at the end of the section, see Corrections \ref{second_derivative:haralick:statement} and \ref{second_derivative:haralick:mask_computation}.\\

Also, the explanations are badly organized:
\que  The least-square problem is not "approximately" solved but "EXACTLY" solved with the pseudo-inverse.\\

\ans Agreed. Corrected now.\\


\que $- (A^TA)^{−1}A^T f=k \Rightarrow k=Bf$ should be rephrased as
$k=Bf$ with $B=(A^TA)^{−1}A^T$\\

\ans We didn't follow exactly the suggestion, but changed the text in the same spirit. See \refc{second_derivative:haralick:normal}.

\que Equation 3.4 and the one just before it should be removed since they are
simple reformulation of the matrix equation above. At best you can keep the
notations for the entries of B that is $b_{i,j}$ for $i\in {1,\ldots,10}$ and
$j \in {1,ldots,25}$.\\

\ans While we agree that there is some superfluous and simple equations, we found that those tricks to speed up are quite hard to follow at first. Thus, this level of detail could help the reader to clearly understand whats going on.\\

\que the introduction of $\bf{b_i}$ comes too soon, they corresponds to the masks of table 3.1\\

\ans We rearranged the explanations in this section. In particular now $\bf{b_i}$ is well defined, and we introduced a new variable $\mathbf{mask_i}$ which corresponds to the matrix for of $\bf{b_i}$ and postponed the introduction of these masks to a point closer to table 3.1. However that table was somehow superfluous and thus we moved it to an appendix, to avoid adding more noise to the text.\\

\que  I would rephrase all the explanations that follows the definition of $\bf{b_i}$
focusing on the purpose of speed up; I would first explain that for each
$i\in {1,\ldots,10}$ one can store all the $k_i$'s in an image, and that this
image can be obtained by a convolution with a mask of the form $\bf{bi}=...$
And then I would add that each mask $\bf{b_i}$ can be precomputed once for all and are provided in table 3.1.\\

\ans We followed this suggestion, which in our opinion is a great improvement for the presentation. See \refc{second_derivative:haralick:mask_computation} Thanks to the reviewer for this one. 

\que Can you explain how you have obtained the entries of the mask? If you used a program for that, I think you should provide it.\\

\ans XXX\\

\subsubsection{Section 3.2.2: Analytical calculation of the second derivative}

\que This description is unclear to me. Does the described procedures allows to
compute the laplacian operator (under the bicubic approximation)? If yes,
explain why this is the case. By reading the original paper by Haralick, I
could determine that the answer is negative, and that the goal is to compute a second derivative in the direction of the gradient. The only sentence referring to this purpose is at the very end of the section:
"Given the direction defined by θ, with ρ > 0, an edge will always be an
ascending step. This indicates that the first derivative in this direction has
a maximum and that the second derivative has a zero crossing with negative
slope. That is, the third derivative less than zero."
If so I would recommend to put more emphasis on this goal and the justification that I quoted from your text. And you should definitely place the
corresponding sentences at the very beginning (before giving the mathematical details).\\

\ans Changed as suggested. See \refc{second_derivative:haralick:analitical:statement}.\\

\que Using the expression "polar form" is completely misleading since theta is
fixed. Haralick simply considers the second derivative in the direction of the
gradient and there comes the restriction $f_\theta(\rho)$ of the polynomial
approximant to the line of angle theta.\\

\ans Changed, see \refc{second_derivative:haralick:analitical:derivative}. We now explicitely defined the line of angle theta, and described the points lying on that line in terms of $\rho$ and $\theta$\\

\que Equation 3.9 is not mathematically correct. You should add before it, $\exists
\rho\in[0,\rho_0]$ s.t. Equation 9.\\

\que I would integrate the footnote 7 within the text, just after: "This parameter
must be greater than zero and less
than 1." since it provides the rationale behind these constraints.\\

\ans Done, see \refc{second_derivative:haralick:analitical:parameter_value}.\\

You should explain what you mean by the optimal value of $\rho_0$. If you are
simply referring to Haralick's claims, you should state it explicitly.

Also, are you sure you implement the exact conditions specified in the
original article by Haralick. The ones I could find are as follows:
"If for some $\rho, |\rho|\leq\rho_0, [...], f"_\alpha(\rho)<0$ [eq.1],
$f''_alpha(\rho)=0 [eq.2] and f'(\rho)\neq 0$ [eq.3] we have discovered [...]"
I guess that [eq.1] was meant to involve a third derivative instead of a second
derivative. Then it is exacly your condition $c_3<0$. I don't find anything
close to [eq.3] in your description. Besides, although (as you justified it)
[eq.2] implies your condition $|\frac{C_2}{3C_3}|\leq\rho_0$ they are not
equivalent. I would say that you should add that $C_2$ must be positive as well. If you want to stick with your conditions only, you should at least add a discussion pointing on the fact that they differ from the one formulated by Haralick, and
describe the differences. I actually tried changing your code to take the
sign of $C_2$ into account and to introduce [eq.3]: it does not seem to change
the output. Yet I would recommend that you investigate why and provide that
information.

\subsubsection{ Section 4: Common Mathematical Operations}
"All algorithms implemented" --> "All implemented algorithms"

\subsubsection{ Figure 11:}
"Is clearly required" --> "It ..."

The text in the figure is practically unreadable.


\subsubsection{ Section 5: Results}
The experimental discussion is quite succinct, I would expect Ipol articles to
be more in-depth. For instance, it seems important that the impact of the
algorithms parameters be studied more deeply with dedicated examples and that
you give some hindsight on the way you choose their values (why did you pick
$th_zc=0.1$ for one experiment and then 0.13 in the other one). I would
also expect the claims of the original
articles to be challenged, confirmed or denied. As an example, Haralick claimed
that his algorithm performed way better than the one by Marr and Hildreth.

\que You use too many short paragraphs!! This is true in general for your text, but
it is especially pronounced here. I would recommend one paragraph for the synthetic example and one or two for the windmill example.\\

\ans XXX\\

\que Also, I don't see the point of duplicating the execution time data, once in the figures and another time in a dedicated table.\\

\ans Since we don't have any space restrictions, summarizing all execution times in a table, will help the reader compare them.

I am not entirely convinced by your explanation of the thicker edges in
Haralick case. There are two reasons form my doubts:
- My first point is that even with a smooth model, the fact that the detection
is based on zero-crossing of second derivative should make the detection well localized. Indeed when dealing with a smooth edge, on the contrary to the
first derivative methods, zero-crossing is supposed to fire only at the
location of maximum gradient.
- Then, taking the opposite view to your argument, imagine that you would fit a more regular polynomial, let's say of degree 4. Then instead of getting a
smoother approximant, I imagine that you would witness some kind of Runge's
phenomenon which would translate in ringing patterns that would make the edge detection less predictable. It may be that the reason of the thicker edges is precisely the ringing that would create many local edges. But I must say that
this is only a lead and I am not sure of this explanation either. You could
challenge this argument on the synthetic example by plotting the profiles of
the function $f_theta(rho)$ at few edge locations.


\subsubsection{ Section 6: Conclusion}
Your conclusion is quite poor. It simply recaps what was just stated in the
experimental section. You could try to improve it by placing the
algorithms under study in a broader context. The only part of your conclusion
that follows that path is the reference to Canny's approach. Much more
works have been made on edge detection since then, for instance approaches
based on machine learning that have several advantages to be discussed. This one option among others.

Again, too many short paragraphs.

\que "runs significative faster that" --> "runs significaNTLY faster thaN"\\

\ans corrected\\

\section{ Comments on the Code and demo}

\subsubsection{General description of the code}
The reviewed code consists of two files: edges.c contains the main function,
while the actual algorithms are included in \verb+classic_edge_detectors.c+; This
latter contains several routines:
- 6 routines are dedicated to each algorithms or their different flavors.
Accordingly one can find the following routines:
\begin{verbatim}
+ edges_roberts
+ edges_prewitt
+ edges_sobel
+ edges_mh for the Marr-Hildreth algorithms with gaussian and laplacian
+ edges_mh_log for the same algorithm with the laplacian of gaussian
+ edges_haralick
- another routine called conv2d implements image filtering with a squared kernel
- eventually two other are responsible for generating a gaussian kernel or laplacian of
gaussian
\end{verbatim}


The implementations of all the detectors match faithfully their description in
the pdf. This is obviously a very good point. I checked as carefully as I could
all the kernel entries. This was quite some fun concerning Haralick's mask!
Although in this case I only checked that the entries were identical in the pdf
and the code (I did not check the mathematical validity of those entries).

Consistently to my expectation, the three routines corresponding to first order
derivative (robert, prewitt and sobel) are strictly identical (except for
names of variables and the kernel entries). One could argue that this property,
that is quite clear in the paper, should have been reflected in the code. Yet I
would understand that the authors prefer to keep the three routines independent
from one another.

Similarly, the two routines implementing the two flavors of the Marr-Hildreth
detector are entirely identical a part from the filtering phase. Again both
implementation could be merged into a single routine.

\subsection{ Algorithm's parameters}
From the article I could retrieve the following list of parameters:
\begin{verbatim}
- First order methods : th
- Marr-Hildreth: sigma, n, $th_{ZC}$
- Haralick: $ro_0$
- All algorithms: padding methods
\end{verbatim}
They corresponds exactly to the ones present in the code and (except for the
padding method) also those exposed in the demo. This is quite satisfying!

\subsection{ Minor comments}
\subsubsection{ In gaussian\_kernel}
The normalization by the sum is different from the one evoked in Section
4.1.1. You should either change the implementation or comment on this in the article. Note
also that the comment in the code correspond to the formula in the pdf instead of the implemented one.


\subsubsection{ In LoG\_kernel }
The same problem of consistency for the normalization occurs here. It is in
fact worse than in the gaussian case, since I suppose that the LOG kernel does not sum to one. Besides dividing by $sigma^4$ is pointless if you normalize by the sum! Of course, kernel normalization has no impact on the edge detection since the thresholds are expressed in a relative way.

\subsubsection{ In the README:}
\que you announce that:
"More than one algorithm can be applied to the same image with the same
command. For example:
edges -r 0.1 -l 3 29 0.13 molino.png
will apply Roberts' algorithm with threshold parameter 0.1, and the
Marr-Hildreth (LoG) with parameters sigma=3.00, N=29, TZC=0.13;"
You should also warn that each algorithm can be launched only once. For instance
the following call will fail:
edges -r 0.1 -r 0.2 -l 3 29 0.13 molino.png

\section{ Conclusion}
Overall I think the submission is a good candidate for IPOL but it cannot be
accepted in the current state. My main complain relates to the poor quality of
the PDF:
\begin{verbatim}
- many explanations are unstructured
- the writing is often bad (I am under the impression that the pdf was submitted without
proofreading)
- Except for the algorithmic description that is sufficiently detailed, many
aspects are lacking: specifically in the experimental part and the conclusion.
\end{verbatim}


\bibliographystyle{unsrt}
\bibliography{../../tools/journal_list_short,../../tools/references,../../tools/hierarchical_segmentation}
\end{document}


 